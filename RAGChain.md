## Documentation for RAGChain

### Overview

The `RAGChain` class implements a system for **Retrieval-Augmented Generation (RAG)**, combining a retrieval-based method using FAISS (Facebook AI Similarity Search) for document indexing and retrieval, with OpenAI's GPT language models to provide accurate, context-driven responses. This system is particularly designed for answering user questions based on the contents of a PDF document.

### Architecture

The architecture of the `RAGChain` can be divided into three key parts:
1. **PDF Parsing**: Extracting text from the PDF using `PyPDF2`.
2. **Chunking and Vectorization**: Dividing the extracted text into manageable chunks and creating embeddings using OpenAI models, stored in a FAISS vector store for efficient retrieval.
3. **Question-Answering Process**: Using OpenAI's GPT models for question reformulation and response generation based on retrieved context from the FAISS vector store.

### Key Components

#### 1. **PDF Text Extraction (`extract_text_from_pdf_with_pypdf2`)**
   - **Purpose**: To extract text from the given PDF file.
   - **Method**: The `PyPDF2.PdfReader` is used to read the PDF file and extract the text content from each page.
   - **Input**: A file path to the PDF document.
   - **Output**: A string containing the extracted text.

#### 2. **Text Chunking (`create_chunks_for_pypdf2_parse`)**
   - **Purpose**: To split the extracted PDF text into smaller, more manageable chunks for easier processing and retrieval.
   - **Method**: Uses `RecursiveCharacterTextSplitter`, which divides text into chunks of a specified size (in this case, 1000 characters) without overlap.
   - **Input**: Extracted text from the PDF.
   - **Output**: A list of text chunks, each of up to 1000 characters in length.

#### 3. **FAISS Vector Store Creation (`create_vectorstore_with_faiss`)**
   - **Purpose**: To create a vector store for efficient document retrieval.
   - **Method**:
     - Embeddings are generated from the text chunks using OpenAI embeddings (`OpenAIEmbeddings`).
     - These embeddings are then stored in a FAISS vector store, which allows for fast similarity-based search when the user poses a query.
   - **Input**: The chunked text data.
   - **Output**: A FAISS vector store object for later querying.

#### 4. **LLM Initialization**
   - **Purpose**: The `ChatOpenAI` class from the OpenAI library is used to create the GPT model that will be responsible for answering questions.
   - **Model**: `gpt-4o-mini` is specified for this instance of the GPT model, with an API key provided via environment variables (`OPENAI_API_KEY`).

#### 5. **Question Handling (`ask_question`)**
   - **Purpose**: To process a user's question and return a contextually accurate answer.
   - **Steps**:
     1. **Chat History Aggregation**: Compiles the recent conversation history to provide context to the language model.
     2. **Question Reformulation**: The GPT model generates a single concise question based on both the chat history and the user’s query.
     3. **Context Retrieval**: The FAISS vector store retrieves relevant document chunks based on the reformulated question. This is done by converting the question into an embedding and querying the vector store for the most relevant chunks.
     4. **Answer Generation**: GPT is used again to generate an answer to the reformulated question using the retrieved chunks as context.
     5. **Updating Chat History**: The question and the response are stored in the chat history to maintain continuity across user interactions.
   - **Input**: User's question.
   - **Output**: The answer generated by the LLM, along with the context that was used to generate it.

#### 6. **Clear Chat History (`clear_history`)**
   - **Purpose**: Clears the chat history, allowing for a fresh start in conversations.

#### 7. **Retrieve Chat History (`get_chat_history`)**
   - **Purpose**: Retrieves the entire chat history, which stores user questions and assistant responses, allowing for contextual responses in subsequent interactions.

### Workflow

1. **PDF Loading**: The PDF file is loaded and parsed using `extract_text_from_pdf_with_pypdf2`.
2. **Text Chunking**: The parsed text is split into manageable chunks using `RecursiveCharacterTextSplitter` to create chunks of 1000 characters each.
3. **Vectorization**: The chunks are then vectorized (embedded) using OpenAI embeddings and stored in a FAISS vector store for efficient retrieval.
4. **Question Processing**: When the user asks a question, the chat history is first used to maintain context.
   - A reformulated question is generated using the GPT model.
5. **Context Retrieval**: The reformulated question is passed to the FAISS vector store, which returns the most relevant document chunks.
6. **Response Generation**: The GPT model generates an answer to the question based on the retrieved document chunks and the user query.
7. **Chat History Update**: Both the user’s question and the generated answer are saved in the chat history.

### Retrieval Approach

The retrieval process relies on FAISS, which allows for efficient similarity search across high-dimensional vectors. The document chunks are first embedded into vector representations using OpenAI’s embedding models, and then stored in the FAISS index.

When a user poses a question:
- The question is first passed through the LLM for reformulation based on the chat history.
- This reformulated question is embedded and passed to the FAISS index, which retrieves the most relevant chunks from the document.
- These retrieved chunks are then used as context for the LLM to generate a response.

### Generative Response Creation

The generative response is handled by OpenAI’s GPT model, which leverages both:
1. **The User's Question**: The original user input is provided to ensure the response is aligned with the question.
2. **Retrieved Context**: The retrieved document chunks provide the GPT model with specific information needed to answer the question.

The model takes into account the chat history, context retrieved from the FAISS store, and the user’s current query to produce a response that is coherent and factually grounded in the provided PDF content.

### Use Cases

This architecture is highly effective for:
- Answering questions based on large documents like research papers, books, legal documents, or manuals.
- Providing factual responses from specific sections of a document, eliminating the need for a user to manually search through the document.
- Supporting interactive conversations with context, ensuring consistency in dialogue over time.

### Dependencies

- **PyPDF2**: For reading and extracting text from PDF files.
- **FAISS**: For efficient similarity-based search on embedded vectors.
- **Langchain**: For orchestrating document retrieval, text splitting, and managing LLM interactions.
- **OpenAI GPT**: For generative responses and embedding creation.
- **dotenv**: For loading the OpenAI API key from environment variables.

### Conclusion

The `RAGChain` class combines document retrieval techniques with powerful generative models, enabling users to ask questions and receive factually grounded responses based on the contents of a PDF. It is designed to efficiently handle large documents, retrieve relevant information, and maintain conversational context for better user experiences.